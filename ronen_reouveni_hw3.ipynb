{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3.8.3 64-bit",
      "language": "python",
      "name": "python38364bit99d3e1796ec945bf83a72ade385ef7bd"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "colab": {
      "name": "ronen_reouveni_hw3.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pWvuCAtAhGAJ"
      },
      "source": [
        "# Ronen Reouveni \n",
        "\n",
        "# NLP Homework 3: Sentiment Classification \n",
        "\n",
        "---\n",
        "\n",
        "## Introduction \n",
        "\n",
        "In this analysis we train a sentiment classifier on airline tweets. We then use these models on a set of covid tweets and analyze the resulting sentiment distributions. Each tweet in the airline dataset is annotated to have labels of either negative, positive, or neutral sentiment. However, the covid tweets have no labels whatsoever. Therefore, all evaluation metrics are calculated using cross validation within the airline tweets. The hope is that we can generlize the model to the covid tweets. Although we will be able to analyze how many sentences in the covid tweets are classified with a given sentiment, we will not know the classification rate because the covid tweets have no annotated labels. \n",
        "\n",
        "\n",
        "## Explanation of Cleaning, Preprocessing, and Analysis \n",
        "\n",
        "### Naive Bayes \n",
        "\n",
        "To begin the cleaning and preprocessing, we remove the '@', '#', and digits from all the tweets in the airline corpus. The object is to classify sentiment on the sentence level of the covid tweets using the models trained on the airline tweets. Therefore, I then iterate over the covid tweets and append each sentence to a new list. An inner loop in this iteration tracks the author and country. This is necessary because if a tweet is 3 sentences it has the same author, but it should be split into 3 sentences. Therefore, the author needs to be associated with each sentence. A similar logic applies to country and title. I then ensure we have the correct data shapes. Only a quarter of the sentences in the covid dataset are used. I extract the labels of the airline tweets, create a list of airline tweets and a list of sentences from the covid set. \n",
        "\n",
        "At no point do I remove stopwords or lowercase any text. Although there are still very interesting results lowercasing would have improved them. This is discussed more in the final analysis and interpretation. \n",
        "\n",
        "---\n",
        "\n",
        "Two separate feature sets are created for two separate Naive Bayes models. I use CountVectorizer from sklearn to both tokenize and obtain 'bag of words'. To be clear, the bag of words is created on the entire dataset, both the covid tweets and airline tweets. The amazing thing about CountVectorizer from sklearn are the parameters that can be passed into it as well. The two used in this analysis is ngram_range and min_df. ngram_range describes the minimum and maximum ngram size created. For example, passing in 1,1 means we will only get unigrams, 1,2 means we will get unigrams and bigrams, 1,3 means unigrams, bigrams, and trigrams. Furthermore, 2,3 means we only will get bigrams and trigrams. The second parameter is min_df. This defines the minimum number of documents that the n_gram can be in or it will be excluded. \n",
        "\n",
        "\n",
        "1. Complement Naive Bayes:\n",
        "    * Unigrams \n",
        "    * minimum document number: 2000\n",
        "    * resulting features: 891\n",
        "\n",
        "2. Complement Naive Bayes:\n",
        "    * Unigrams, Bigrams, Trigrams\n",
        "    * minimum document number: 1000\n",
        "    * resulting features: 2707\n",
        "    \n",
        "The CountVectorizer is run on both corpuses together. This means that the n_grams found are across all the data. Furthermore, the minimum document count includes all the data, both corpuses. However, when it comes time to train the Naive Bayes models, it is only trained on the data from the airline tweets. The reason the count vectorization is done with all the data is so we can truly generalize between the two. However, we only train on the airline set because it is the only set that has labels. I accomplish this by subsetting my dataframes’ by index where the index reflects the ending row number of the airline tweet data set. \n",
        "\n",
        "\n",
        "\n",
        "### Deep RNN: LSTM \n",
        "\n",
        "The preprocessing required to run the LSTM is more involved than what we need to do for the Naive Bayes modeling. We need to create a tokenizer with a limit of vocabulary. This is the vocabulary that the model will hold. This is set to 7,500 words. This means that there are 7,500 words that the model will remember and hold. We then need to create a padded sequence with a max length. This processing converts words to numbers, like a dictionary key pair values. However, it maintains the order of the words. The max length is set to 200. This means that we analyze a window of 200 sequential words. Because language is sequential in nature we need a model that does not degrade the order of the words. This is a major improvement from the Naive Bayes models. The way that we are able to do this is by using an RNN, a Recurrent Neural Network. Specifically I use an LSTM. \n",
        "\n",
        "The LSTM implemented here also uses dropout. This randomly turns off a portion of the neurons to prevent overfitting. The final layer of the model is a dense fully connected layer with 3 outputs. It has a sigmoid activation function. This final layer has three nodes because we are looking for three outputs, positive, neutral, or negative. The LSTM will output a probability for each class. The classification is the node with the highest probability. We use SparseCategoricalCrossentropy as the loss function. This is because we have unbalanced data with 3 classes. Furthermore, we use 'adam' optimizer. \n",
        "\n",
        "The actual architecture of the LSTM implemented in this analysis is as followed.\n",
        "\n",
        "Model: \"sequential_6\"\n",
        "_________________________________________________________________\n",
        "-----------------------------------------------------------------\n",
        "embedding_6 (Embedding)      (None, 200, 32)           504608    \n",
        "_________________________________________________________________\n",
        "spatial_dropout1d_6 (Spatial (None, 200, 32)           0         \n",
        "_________________________________________________________________\n",
        "lstm_6 (LSTM)                (None, 50)                16600     \n",
        "_________________________________________________________________\n",
        "dropout_6 (Dropout)          (None, 50)                0         \n",
        "_________________________________________________________________\n",
        "dense_6 (Dense)              (None, 3)                 153       \n",
        "\n",
        "\n",
        "* Total params: 521,361\n",
        "* Trainable params: 521,361\n",
        "* Non-trainable params: 0\n",
        "\n",
        "\n",
        "As we can see above, there are 521,361 parameters that are trained and optimized. \n",
        "\n",
        "\n",
        "## Explanation of Analysis\n",
        "\n",
        "Cross validation will be used on both Naive Bayes models to calculate accuracy scores. The best Naive Bayes model will then be chosen and we will use that model to predict the sentiment of the sentences in the covid tweets set. From here, we can get counts of how many sentences are neutral, positive, and negative. Furthermore, I can extract top adverb, adjective, and verb phrases from both the positive and negative tweets.\n",
        "\n",
        "Another interesting mode of analysis is looking at the conditional mean for each sentiment classification given the country it comes from. This will highlight which countries have the highest positive sentiment and negative sentiments. However, average sentiment is not simple to interpret because of how the data is structured. The way that I execute this is by taking conditional mean count of positive and negative sentences per country. These are the predicted labels by the Naive Bayes model. Then I take the difference between positive and negative. If the result is more negative, the sentiments on average, given that specific country, is more negative. If the number is positive, given that specific country, the sentiment is on average more positive. \n",
        "\n",
        "I will also test the LSTM on some fake sentences to try and understand its nature. After getting an idea of its behavior I will randomly select covid tweets that are labeled as positive and negative by the Naive Bayes and then reclassify them using the LSTM. The hope is to compare tweets classified with opposite sentiments by each classifier. \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "na_rrEfchGAP"
      },
      "source": [
        "## Code and Output\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4w4Kv4cnhGAR"
      },
      "source": [
        "#import data\n",
        "import pandas as pd \n",
        "import numpy as np\n",
        "train_dataset = pd.read_csv('train_tweets_airlines.csv') \n",
        "test_set = pd.read_csv('test_covid.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X2AF8gLbhGAS"
      },
      "source": [
        "import re\n",
        "import nltk\n",
        "# Since this is social media data, we will have to add a few extra preprocessing steps\n",
        "# First, let's remove  @ and # (Twitter platform affordances) from the training data\n",
        "# We'll use regular expressions for that, creating a Function that we can use to pass the data through\n",
        "pattern = r'[0-9]'\n",
        "\n",
        "def remove_at(x):\n",
        "    x = str(x).replace('@', '')\n",
        "    x = str(x).replace('#', '')\n",
        "    x = re.sub(pattern, '', x)\n",
        "    return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mcLyaHUYhGAS"
      },
      "source": [
        "# Clearning the data with our function\n",
        "textList = list(train_dataset['text'].apply(lambda x: remove_at(x))) #call above function on text \n",
        "\n",
        "\n",
        "textList_test = list(test_set['text'].apply(lambda x: remove_at(x))) #unused\n",
        "\n",
        "sentList = list(train_dataset['airline_sentiment']) #extract sentiment into new list "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AFzVVFe_hGAT"
      },
      "source": [
        "#split covid set into sentences\n",
        "#save the author, country, title of each sentence into new lists\n",
        "\n",
        "sentences = []\n",
        "authors = []\n",
        "country = []\n",
        "title = []\n",
        "\n",
        "for i in range(0, int(len(test_set['text'])/4)): #select quarter of data \n",
        "    container = (nltk.sent_tokenize(test_set['text'][i])) #tokenize each tweet by sentence \n",
        "    sentences.append(container) #append sentences \n",
        "    for k in range(0, len(container)): #append data for each sentence \n",
        "        authors.append(test_set['author'][i])  \n",
        "        title.append(test_set['title'][i])\n",
        "        country.append(test_set['country'][i])\n",
        "\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r5bChtlFhGAT"
      },
      "source": [
        "import itertools\n",
        "#fix data structure created in the previous code, itertools \n",
        "sentences = (list(itertools.chain.from_iterable(sentences)))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dvYorr7VhGAT",
        "outputId": "05e98649-a59c-4395-acaa-a434567a55b3"
      },
      "source": [
        "#ensure the lengths all match up \n",
        "print(len(sentences))\n",
        "print(len(authors))\n",
        "print(len(country))\n",
        "print(len(title))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "583450\n",
            "583450\n",
            "583450\n",
            "583450\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vHvH66MThGAU",
        "outputId": "f279973f-a844-404e-ff76-fd7c8914c091"
      },
      "source": [
        "#ensure the first sentence list has correct dimensions \n",
        "print(len(textList))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "14640\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wLa1M81lhGAV",
        "outputId": "d18d1adc-fc9e-4b1c-fc33-355ba2a22de4"
      },
      "source": [
        "#create list of all data from both airline and covid \n",
        "totalList = textList + sentences\n",
        "print(len(totalList))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "598090\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9PvIfVqPhGAV"
      },
      "source": [
        "#get grams \n",
        "from sklearn.feature_extraction.text import CountVectorizer #import module \n",
        "vec = CountVectorizer(ngram_range=(1,1), min_df = 2000) #create object with unigrams and 2000 min doc size \n",
        "X = vec.fit_transform(totalList) #apply the object to ALL tweets/sentences \n",
        "df = pd.DataFrame(X.toarray(), columns=vec.get_feature_names()) #save in a dataframe "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dmrqv2_NhGAV",
        "outputId": "b1b8d9c4-3555-474e-a9b9-41a7109b9c36"
      },
      "source": [
        "#the shape of the df ensures we have the correct rows \n",
        "#14640+583450 = 598090\n",
        "\n",
        "#note that the above code left us with 891 predictors \n",
        "df.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(598090, 891)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tjyZobDihGAW",
        "outputId": "d3f0cb01-88aa-4e31-8068-29138dbf5334"
      },
      "source": [
        "from sklearn.model_selection import cross_val_score #import cross validation \n",
        "from sklearn.naive_bayes import ComplementNB        #import model\n",
        "clf = ComplementNB() #instantiate model \n",
        "\n",
        "#train the classifier on ONLY the airline tweets\n",
        "#notice we have 14640 rows (airline set), and the sentList, the extracted sentiment labels \n",
        "#note: this automatically randomizes the rows \n",
        "scores = cross_val_score(clf, df[0:14640], sentList, cv=5) #call model with 5 folds \n",
        "scores"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.67588798, 0.64378415, 0.60382514, 0.72096995, 0.70696721])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ObE_QczMhGAW",
        "outputId": "022fb8fe-b134-4bab-b81f-79ead7dd618a"
      },
      "source": [
        "#cv score of only .67\n",
        "sum(scores)/5"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6702868852459016"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rlt06wPQhGAX"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DvK-wlr1hGAX"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aAf0n_6IhGAX"
      },
      "source": [
        "#repeat same process as above but with the addition of bigram, and trigram\n",
        "#also note new minimum of 1000 documents \n",
        "vec = CountVectorizer(ngram_range=(1,3), min_df = 1000)\n",
        "X = vec.fit_transform(totalList)\n",
        "df = pd.DataFrame(X.toarray(), columns=vec.get_feature_names())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bLaWcWw-hGAX",
        "outputId": "e68df6be-cc95-434c-c588-9acc5f429f0d"
      },
      "source": [
        "# now we have 2707 columns \n",
        "df.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(598090, 2707)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rhu_45u_hGAY",
        "outputId": "cea35f2c-ced3-4bca-ec6b-b879bfe4d882"
      },
      "source": [
        "#train new NB on ONLY the airline tweets \n",
        "#note: this automatically randomizes the rows \n",
        "clf_bi = ComplementNB()\n",
        "scores_bi = cross_val_score(clf_bi, df[0:14640], sentList, cv=5)\n",
        "scores_bi"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.69740437, 0.6875    , 0.67657104, 0.76058743, 0.73155738])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HqQRp2jUhGAY",
        "outputId": "84f39240-1285-412b-fb97-80027b586c58"
      },
      "source": [
        "#new score is up to .71 with unigrams, bigrams, and trigrams \n",
        "sum(scores_bi)/5"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7107240437158471"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GR4fh6mUhGAY"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C7uRB5PZhGAY"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7yGoyySzhGAY",
        "outputId": "fd5e99ca-7ee4-4684-ccbb-9b17f2490a5e"
      },
      "source": [
        "#re-fit the better model with the data from the airline set \n",
        "clf_bi.fit(df[0:14640], sentList)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ComplementNB()"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u3Jl4II0hGAZ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ykYmt4ejhGAZ"
      },
      "source": [
        "#make predictions on the covid set using the NB model \n",
        "testPredictions = clf_bi.predict(df[14640:598090])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u70JvY9bhGAZ",
        "outputId": "be33fd40-e16b-4352-e668-aaf0102b2334"
      },
      "source": [
        "#ensure the number of predictions made matches the expected\n",
        "len(testPredictions)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "583450"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ut_rzmMthGAZ"
      },
      "source": [
        "#get index locations of pos, neg, and nue sentences\n",
        "indices_pos = [i for i, x in enumerate(testPredictions) if x == \"positive\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5yeUf6BKhGAa"
      },
      "source": [
        "indices_neg = [i for i, x in enumerate(testPredictions) if x == \"negative\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aDKOrqu0hGAa"
      },
      "source": [
        "indices_nue = [i for i, x in enumerate(testPredictions) if x == \"neutral\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hdm5Fl4xhGAa",
        "outputId": "9fecf65f-5920-43d7-cd9f-72031fecce18"
      },
      "source": [
        "#view counts \n",
        "len(indices_pos)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "171743"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PigFNagJhGAa",
        "outputId": "de15a3cd-d3f6-472c-fcf9-4b0a21b58922"
      },
      "source": [
        "#view counts \n",
        "len(indices_neg)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "107066"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EHAaN2EvhGAa",
        "outputId": "96fb20ce-944a-469c-f04d-2f4c2047b230"
      },
      "source": [
        "#view counts \n",
        "len(indices_nue)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "304641"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zHGwlsiphGAb",
        "outputId": "2f019a96-ef45-42af-b689-432cee7a0d9b"
      },
      "source": [
        "#make sure it adds back up to expected\n",
        "sum([217952,229193,136305])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "583450"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3HG6OzEGhGAb"
      },
      "source": [
        "#loop through sentence list and append to a new list \n",
        "pos_sents = []\n",
        "\n",
        "for i in indices_pos:\n",
        "    pos_sents.append(sentences[i])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C95edWavhGAb",
        "outputId": "b8df5059-d40c-47a8-ae29-c14bba2f3e8c"
      },
      "source": [
        "#ensure num of pos sentences matches len of indicies \n",
        "len(pos_sents)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "171743"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dn_TmogkhGAb",
        "outputId": "ba8c1075-7b2d-4ba9-83e8-7ad7269fd8c1"
      },
      "source": [
        "#look at first positive sentence \n",
        "pos_sents[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Rajiv Gandhi Institute of Chest Diseases (RGICD) with 15 beds and Wenlock Hospital at Mangaluru with 10 beds have been selected for the treatment of the virus.'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VBPwUVUNhGAc"
      },
      "source": [
        "#fix country data type \n",
        "country = list(country)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LEDSTA7AhGAc"
      },
      "source": [
        "#zip and join all in df \n",
        "subDF = pd.DataFrame(list(zip(title, authors, country, testPredictions)), columns =['title', 'author','country','sentiment'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9N7VP_6QhGAc"
      },
      "source": [
        "#turn sentiment labels using one hot encoding \n",
        "subDF = pd.get_dummies(subDF, columns=['sentiment'])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gffMfpD6hGAc"
      },
      "source": [
        "#group by to get appropriate table \n",
        "finaldf = subDF.fillna('none').groupby(['title','author', 'country'], sort = False).sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P5yU376LhGAc",
        "outputId": "360addc4-7d11-4d15-eb49-fa70ae89d8a6"
      },
      "source": [
        "#view table \n",
        "finaldf"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th>sentiment_negative</th>\n",
              "      <th>sentiment_neutral</th>\n",
              "      <th>sentiment_positive</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>title</th>\n",
              "      <th>author</th>\n",
              "      <th>country</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Karnataka: Helplines, isolation wards set up for coronavirus - Udayavani</th>\n",
              "      <th>Udayavani</th>\n",
              "      <th>IN</th>\n",
              "      <td>1.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Health dept. monitoring 24 people for possible infection</th>\n",
              "      <th>none</th>\n",
              "      <th>US</th>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>none</th>\n",
              "      <th>jmccorm</th>\n",
              "      <th>US</th>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Asian Markets Mostly Higher</th>\n",
              "      <th>rttnews.com</th>\n",
              "      <th>US</th>\n",
              "      <td>1.0</td>\n",
              "      <td>16.0</td>\n",
              "      <td>5.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Tesla soars as bearish analysts left with little to highlight - BNN Bloomberg</th>\n",
              "      <th>Joe Easton</th>\n",
              "      <th>CA</th>\n",
              "      <td>3.0</td>\n",
              "      <td>14.0</td>\n",
              "      <td>7.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <th>...</th>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Coronavirus: expert warns infection could reach 60% of world's population | World news | The Guardian</th>\n",
              "      <th>Sarah Boseley</th>\n",
              "      <th>US</th>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Viruses don’t care if you’re lying or not</th>\n",
              "      <th>Neil Steinberg</th>\n",
              "      <th>US</th>\n",
              "      <td>29.0</td>\n",
              "      <td>26.0</td>\n",
              "      <td>13.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Australian Dollar Edges Higher as Aussie Home Loans Beat Forecasts</th>\n",
              "      <th>Colin Lawrence</th>\n",
              "      <th>GB</th>\n",
              "      <td>1.0</td>\n",
              "      <td>22.0</td>\n",
              "      <td>4.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Eight days in Wuhan, cut off from the world</th>\n",
              "      <th>none</th>\n",
              "      <th>ZA</th>\n",
              "      <td>25.0</td>\n",
              "      <td>22.0</td>\n",
              "      <td>24.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Aero-News: Quote of the Day (02.11.20)</th>\n",
              "      <th>none</th>\n",
              "      <th>US</th>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>23831 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                                           sentiment_negative  \\\n",
              "title                                              author         country                       \n",
              "Karnataka: Helplines, isolation wards set up fo... Udayavani      IN                      1.0   \n",
              "Health dept. monitoring 24 people for possible ... none           US                      0.0   \n",
              "none                                               jmccorm        US                      1.0   \n",
              "Asian Markets Mostly Higher                        rttnews.com    US                      1.0   \n",
              "Tesla soars as bearish analysts left with littl... Joe Easton     CA                      3.0   \n",
              "...                                                                                       ...   \n",
              "Coronavirus: expert warns infection could reach... Sarah Boseley  US                      0.0   \n",
              "Viruses don’t care if you’re lying or not          Neil Steinberg US                     29.0   \n",
              "Australian Dollar Edges Higher as Aussie Home L... Colin Lawrence GB                      1.0   \n",
              "Eight days in Wuhan, cut off from the world        none           ZA                     25.0   \n",
              "Aero-News: Quote of the Day (02.11.20)             none           US                      0.0   \n",
              "\n",
              "                                                                           sentiment_neutral  \\\n",
              "title                                              author         country                      \n",
              "Karnataka: Helplines, isolation wards set up fo... Udayavani      IN                     5.0   \n",
              "Health dept. monitoring 24 people for possible ... none           US                     2.0   \n",
              "none                                               jmccorm        US                     1.0   \n",
              "Asian Markets Mostly Higher                        rttnews.com    US                    16.0   \n",
              "Tesla soars as bearish analysts left with littl... Joe Easton     CA                    14.0   \n",
              "...                                                                                      ...   \n",
              "Coronavirus: expert warns infection could reach... Sarah Boseley  US                     1.0   \n",
              "Viruses don’t care if you’re lying or not          Neil Steinberg US                    26.0   \n",
              "Australian Dollar Edges Higher as Aussie Home L... Colin Lawrence GB                    22.0   \n",
              "Eight days in Wuhan, cut off from the world        none           ZA                    22.0   \n",
              "Aero-News: Quote of the Day (02.11.20)             none           US                     2.0   \n",
              "\n",
              "                                                                           sentiment_positive  \n",
              "title                                              author         country                      \n",
              "Karnataka: Helplines, isolation wards set up fo... Udayavani      IN                      2.0  \n",
              "Health dept. monitoring 24 people for possible ... none           US                      1.0  \n",
              "none                                               jmccorm        US                      0.0  \n",
              "Asian Markets Mostly Higher                        rttnews.com    US                      5.0  \n",
              "Tesla soars as bearish analysts left with littl... Joe Easton     CA                      7.0  \n",
              "...                                                                                       ...  \n",
              "Coronavirus: expert warns infection could reach... Sarah Boseley  US                      0.0  \n",
              "Viruses don’t care if you’re lying or not          Neil Steinberg US                     13.0  \n",
              "Australian Dollar Edges Higher as Aussie Home L... Colin Lawrence GB                      4.0  \n",
              "Eight days in Wuhan, cut off from the world        none           ZA                     24.0  \n",
              "Aero-News: Quote of the Day (02.11.20)             none           US                      1.0  \n",
              "\n",
              "[23831 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 158
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hHVxzIY2hGAd"
      },
      "source": [
        "#write frame to csv \n",
        "finaldf.to_csv('hm3_nlp_table.csv', index=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Qnma01shGAd"
      },
      "source": [
        "#note: average sentiment is not simple to interpret because of how the data is structured "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ya8ZbhvchGAd",
        "outputId": "6e300d7e-07d5-45e7-a8cc-a927e6ae32a3"
      },
      "source": [
        "#average negative sentiment \n",
        "finaldf['sentiment_negative'].mean()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.49271956695061"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 208
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c9TqnOAWhGAd",
        "outputId": "edb73059-8294-4c3b-c0f3-290b83c17baa"
      },
      "source": [
        "#average positive sentiment \n",
        "finaldf['sentiment_positive'].mean()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "7.206705551592464"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 209
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5CUannY0hGAd",
        "outputId": "d5fd77ae-66b0-4431-a990-06ad60a4d3bd"
      },
      "source": [
        "#get conditional mean sentiments by country \n",
        "avgSentiment = finaldf.groupby('country')['sentiment_negative'].mean()\n",
        "avgSentiment"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "country\n",
              "IN    2.771093\n",
              "US    5.402157\n",
              "CA    5.180972\n",
              "DE    2.404110\n",
              "GB    5.002394\n",
              "        ...   \n",
              "LT    0.000000\n",
              "NP    3.000000\n",
              "RS    6.000000\n",
              "AZ    3.000000\n",
              "KG    1.000000\n",
              "Name: sentiment_negative, Length: 127, dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 212
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PyMfiIcjhGAe",
        "outputId": "874d214d-38c8-47f5-b2b9-72bc2454a4b3"
      },
      "source": [
        "#get conditional mean sentiments by country \n",
        "avgSentiment_neg = finaldf.groupby('country')['sentiment_positive'].mean()\n",
        "index = avgSentiment_neg.index\n",
        "avgSentiment_neg"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "country\n",
              "IN     5.148686\n",
              "US     8.382962\n",
              "CA     7.816960\n",
              "DE     5.047945\n",
              "GB     7.144225\n",
              "        ...    \n",
              "LT     4.000000\n",
              "NP    11.000000\n",
              "RS     5.666667\n",
              "AZ     1.000000\n",
              "KG     0.500000\n",
              "Name: sentiment_positive, Length: 127, dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 215
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zm1KcHkahGAe",
        "outputId": "3dce2d48-0c60-4239-bc3d-592f2730d80f"
      },
      "source": [
        "#create a new frame of the mean difference between pos and neg sentences by country \n",
        "avgSentFrame = pd.DataFrame(list(zip(index, avgSentiment, avgSentiment_neg)), columns = ['country','pos','neg'])\n",
        "avgSentFrame['sent_difference'] = avgSentFrame['pos'] - avgSentFrame['neg']\n",
        "avgSentFrame.sort_values(by = 'sent_difference', ascending = False)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>country</th>\n",
              "      <th>pos</th>\n",
              "      <th>neg</th>\n",
              "      <th>sent_difference</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>BR</td>\n",
              "      <td>10.307692</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>7.307692</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>108</th>\n",
              "      <td>PR</td>\n",
              "      <td>8.000000</td>\n",
              "      <td>5.000000</td>\n",
              "      <td>3.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>87</th>\n",
              "      <td>BT</td>\n",
              "      <td>6.666667</td>\n",
              "      <td>4.333333</td>\n",
              "      <td>2.333333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>72</th>\n",
              "      <td>MU</td>\n",
              "      <td>8.875000</td>\n",
              "      <td>6.625000</td>\n",
              "      <td>2.250000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>116</th>\n",
              "      <td>BZ</td>\n",
              "      <td>5.500000</td>\n",
              "      <td>3.500000</td>\n",
              "      <td>2.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>123</th>\n",
              "      <td>NP</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>11.000000</td>\n",
              "      <td>-8.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>SO</td>\n",
              "      <td>4.555556</td>\n",
              "      <td>14.333333</td>\n",
              "      <td>-9.777778</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>102</th>\n",
              "      <td>PL</td>\n",
              "      <td>8.000000</td>\n",
              "      <td>18.666667</td>\n",
              "      <td>-10.666667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99</th>\n",
              "      <td>CM</td>\n",
              "      <td>6.000000</td>\n",
              "      <td>22.000000</td>\n",
              "      <td>-16.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>BA</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>19.000000</td>\n",
              "      <td>-17.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>127 rows × 4 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "    country        pos        neg  sent_difference\n",
              "35       BR  10.307692   3.000000         7.307692\n",
              "108      PR   8.000000   5.000000         3.000000\n",
              "87       BT   6.666667   4.333333         2.333333\n",
              "72       MU   8.875000   6.625000         2.250000\n",
              "116      BZ   5.500000   3.500000         2.000000\n",
              "..      ...        ...        ...              ...\n",
              "123      NP   3.000000  11.000000        -8.000000\n",
              "22       SO   4.555556  14.333333        -9.777778\n",
              "102      PL   8.000000  18.666667       -10.666667\n",
              "99       CM   6.000000  22.000000       -16.000000\n",
              "30       BA   2.000000  19.000000       -17.000000\n",
              "\n",
              "[127 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 233
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xUjhliGuhGAe",
        "outputId": "14b70a03-bc7c-45d5-c55e-ad906ac28a38"
      },
      "source": [
        "#how does USA look \n",
        "print('Global average',avgSentFrame['sent_difference'].mean())\n",
        "print(avgSentFrame.loc[avgSentFrame['country'] == 'US'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Global average -2.2353136235379365\n",
            "  country       pos       neg  sent_difference\n",
            "1      US  5.402157  8.382962        -2.980805\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sUUtM9sChGAf",
        "outputId": "5e9358d1-1f5d-4953-8fbf-10eec6f74938"
      },
      "source": [
        "#get modules needed \n",
        "import nltk\n",
        "from nltk import sent_tokenize\n",
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /Users/ronenreouveni/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EGr0SYDFhGAf"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PbD02TgVhGAf"
      },
      "source": [
        "#split text and tag\n",
        "tokentext = [nltk.word_tokenize(sent) for sent in pos_sents] #tokenize words within sentences\n",
        "taggedtext = [nltk.pos_tag(tokens) for tokens in tokentext] #apply POS tags to each token generated in the previous line"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cqcf4h-bhGAf"
      },
      "source": [
        "#create function to get phrases \n",
        "def findPhrases(inpuText, phrase):\n",
        "  #define grammer for regex\n",
        "  grammar = phrase\n",
        "  chunk_parser = nltk.RegexpParser(grammar)\n",
        "\n",
        "  #build parse trees\n",
        "  myTags = []\n",
        "  for sent in inpuText:\n",
        "      if len(sent) > 0:\n",
        "        tree = chunk_parser.parse(sent)\n",
        "        for subtree in tree.subtrees():\n",
        "            if subtree.label() == 'test':\n",
        "                myTags.append(subtree)\n",
        "                \n",
        "# append data to list \n",
        "  myPhrase = []\n",
        "  for sent in myTags:\n",
        "    temp = ''\n",
        "    for w, t in sent:\n",
        "        temp += w+ ' '    \n",
        "    myPhrase.append(temp)\n",
        "\n",
        "  mySent = []\n",
        "  for sents in myTags:\n",
        "    temp=''\n",
        "    for (word,tag) in sents:\n",
        "        temp += word+' '\n",
        "        mySent.append(temp)\n",
        "        \n",
        "#calculate average length of phrases\n",
        "  totalSent = sum(len(sent) for sent in mySent) \n",
        "  avgLeng = (totalSent / len(mySent))\n",
        "\n",
        "#calculate frequency distro\n",
        "  freq_ = nltk.FreqDist(myPhrase)\n",
        "  distro = freq_.most_common(50)\n",
        "\n",
        "#turn data into pandas frame\n",
        "  df = pd.DataFrame(distro, columns =['Phrase', 'Freq'])\n",
        "  df.sort_values('Freq', inplace=True)          \n",
        "\n",
        "#return an object with the info needed for analysis\n",
        "  return(distro,df,avgLeng)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A0GJtn2JhGAg"
      },
      "source": [
        "#function to loop through object output from findPhrases and print results \n",
        "def printFreq(obj, title):\n",
        "  print(title)\n",
        "  for word, freq in obj:\n",
        "    print(word, freq)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zSj9S2o8hGAh"
      },
      "source": [
        "adjPhrases = findPhrases(taggedtext,\"test: {<RB.?>+<JJ.?>}\") #\"adjective phrases\" "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ry6WfjIehGAh"
      },
      "source": [
        "#call functions on RB + RB\n",
        "adverbPhrases = findPhrases(taggedtext,\"test: {<RB>+<RB>}\") #\"adverb phrases\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b3btHJrghGAh"
      },
      "source": [
        "#verbs \n",
        "verbverb = findPhrases(taggedtext,\"test: {<VB.?>+<VB.?>}\") #verbs+nouns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pqk8PVVMhGAh",
        "outputId": "161449ab-91b8-4340-fb92-03fe6e964e5d"
      },
      "source": [
        "#combine all into dataframe of tuples with phrase and count\n",
        "df_positive = pd.DataFrame(list(zip(adjPhrases[0], adverbPhrases[0], verbverb[0])), columns =['adjective phrase', 'adverb phrase','verb phrase']) \n",
        "df_positive"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>adjective phrase</th>\n",
              "      <th>adverb phrase</th>\n",
              "      <th>verb phrase</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>(late last , 329)</td>\n",
              "      <td>(so far , 2111)</td>\n",
              "      <td>(have been , 1824)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>(most important , 203)</td>\n",
              "      <td>(as well , 2010)</td>\n",
              "      <td>(has killed , 1778)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>(as much , 167)</td>\n",
              "      <td>(So far , 586)</td>\n",
              "      <td>(has been , 1093)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>(so much , 147)</td>\n",
              "      <td>(not just , 217)</td>\n",
              "      <td>(have tested , 806)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>(too much , 147)</td>\n",
              "      <td>(not only , 201)</td>\n",
              "      <td>(has infected , 768)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>(too early , 143)</td>\n",
              "      <td>(right now , 190)</td>\n",
              "      <td>(have died , 764)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>(so many , 143)</td>\n",
              "      <td>(once again , 180)</td>\n",
              "      <td>(has spread , 745)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>(very early , 143)</td>\n",
              "      <td>(very much , 150)</td>\n",
              "      <td>(have been reported , 591)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>(extremely well-prepared , 129)</td>\n",
              "      <td>(as soon , 124)</td>\n",
              "      <td>(have been confirmed , 587)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>(very good , 121)</td>\n",
              "      <td>(not yet , 122)</td>\n",
              "      <td>(had been , 573)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>(previously unknown , 117)</td>\n",
              "      <td>(as long , 105)</td>\n",
              "      <td>(is believed , 556)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>(more serious , 117)</td>\n",
              "      <td>(very quickly , 100)</td>\n",
              "      <td>(be quarantined , 493)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>(as many , 116)</td>\n",
              "      <td>(very well , 90)</td>\n",
              "      <td>(’ s , 483)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>(most recent , 110)</td>\n",
              "      <td>(so much , 65)</td>\n",
              "      <td>(have been infected , 448)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>(s largest , 94)</td>\n",
              "      <td>(not always , 63)</td>\n",
              "      <td>(has taken , 383)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>(very strong , 89)</td>\n",
              "      <td>(very closely , 62)</td>\n",
              "      <td>(has reported , 379)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>(more severe , 89)</td>\n",
              "      <td>(extremely well , 61)</td>\n",
              "      <td>(was diagnosed , 346)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>(so far killed , 88)</td>\n",
              "      <td>(very hard , 59)</td>\n",
              "      <td>(be taken , 344)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>(very few , 86)</td>\n",
              "      <td>(as much , 52)</td>\n",
              "      <td>(has declared , 343)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>(still unknown , 78)</td>\n",
              "      <td>(not back , 50)</td>\n",
              "      <td>(were reported , 339)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>(still possible , 78)</td>\n",
              "      <td>(first federally , 50)</td>\n",
              "      <td>(are taking , 334)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>(back negative , 76)</td>\n",
              "      <td>(not very , 48)</td>\n",
              "      <td>(was confirmed , 325)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>(seriously ill , 76)</td>\n",
              "      <td>(even just , 46)</td>\n",
              "      <td>(were taken , 322)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>(very much , 74)</td>\n",
              "      <td>(thus far , 45)</td>\n",
              "      <td>(were sickened , 321)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>(only able , 71)</td>\n",
              "      <td>(no longer , 44)</td>\n",
              "      <td>(has sickened , 315)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>(very scary , 70)</td>\n",
              "      <td>(Right now , 43)</td>\n",
              "      <td>(has had , 308)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>(much higher , 68)</td>\n",
              "      <td>(mostly still , 42)</td>\n",
              "      <td>(are working , 281)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>(very important , 68)</td>\n",
              "      <td>(not as , 41)</td>\n",
              "      <td>(were found , 268)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>(much lower , 68)</td>\n",
              "      <td>(definitely very , 39)</td>\n",
              "      <td>(were diagnosed , 260)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>(much more , 67)</td>\n",
              "      <td>(well bid , 39)</td>\n",
              "      <td>(be infected , 252)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>(most popular , 63)</td>\n",
              "      <td>(probably initially , 39)</td>\n",
              "      <td>(have been diagnosed , 251)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>(really good , 63)</td>\n",
              "      <td>(far away , 39)</td>\n",
              "      <td>(had tested , 246)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>(most likely , 62)</td>\n",
              "      <td>(then again , 38)</td>\n",
              "      <td>(have originated , 244)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>(relatively small , 62)</td>\n",
              "      <td>(still relatively , 37)</td>\n",
              "      <td>(was taken , 243)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>(very intense , 61)</td>\n",
              "      <td>(too soon , 35)</td>\n",
              "      <td>(have contracted , 238)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>(more likely , 60)</td>\n",
              "      <td>(yet again , 35)</td>\n",
              "      <td>(has said , 225)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>(s likely , 59)</td>\n",
              "      <td>(together now , 32)</td>\n",
              "      <td>(being treated , 225)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>(most effective , 59)</td>\n",
              "      <td>(not too , 32)</td>\n",
              "      <td>(are going , 215)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>(now higher , 57)</td>\n",
              "      <td>(very close , 31)</td>\n",
              "      <td>(’ re , 212)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>(now more , 55)</td>\n",
              "      <td>(too far , 30)</td>\n",
              "      <td>(are doing , 212)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40</th>\n",
              "      <td>(far lower , 55)</td>\n",
              "      <td>(too much , 29)</td>\n",
              "      <td>(has made , 211)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41</th>\n",
              "      <td>(extremely well prepared , 54)</td>\n",
              "      <td>(not necessarily , 29)</td>\n",
              "      <td>(being infected , 210)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42</th>\n",
              "      <td>(not present , 54)</td>\n",
              "      <td>(very seriously , 28)</td>\n",
              "      <td>(has seen , 205)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>43</th>\n",
              "      <td>(very difficult , 54)</td>\n",
              "      <td>(not immediately , 28)</td>\n",
              "      <td>(has risen , 203)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44</th>\n",
              "      <td>(much larger , 54)</td>\n",
              "      <td>(Even so , 28)</td>\n",
              "      <td>(has become , 203)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45</th>\n",
              "      <td>(very close , 53)</td>\n",
              "      <td>(also well , 28)</td>\n",
              "      <td>(have emerged , 200)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>46</th>\n",
              "      <td>(more confident , 53)</td>\n",
              "      <td>(not long , 28)</td>\n",
              "      <td>(have confirmed , 190)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47</th>\n",
              "      <td>(most common , 53)</td>\n",
              "      <td>(shortly before , 28)</td>\n",
              "      <td>(was put , 186)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>48</th>\n",
              "      <td>(very little , 52)</td>\n",
              "      <td>(n't always , 27)</td>\n",
              "      <td>(has confirmed , 185)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49</th>\n",
              "      <td>(most comprehensive , 52)</td>\n",
              "      <td>(as far , 26)</td>\n",
              "      <td>(had traveled , 177)</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                   adjective phrase              adverb phrase  \\\n",
              "0                 (late last , 329)            (so far , 2111)   \n",
              "1            (most important , 203)           (as well , 2010)   \n",
              "2                   (as much , 167)             (So far , 586)   \n",
              "3                   (so much , 147)           (not just , 217)   \n",
              "4                  (too much , 147)           (not only , 201)   \n",
              "5                 (too early , 143)          (right now , 190)   \n",
              "6                   (so many , 143)         (once again , 180)   \n",
              "7                (very early , 143)          (very much , 150)   \n",
              "8   (extremely well-prepared , 129)            (as soon , 124)   \n",
              "9                 (very good , 121)            (not yet , 122)   \n",
              "10       (previously unknown , 117)            (as long , 105)   \n",
              "11             (more serious , 117)       (very quickly , 100)   \n",
              "12                  (as many , 116)           (very well , 90)   \n",
              "13              (most recent , 110)             (so much , 65)   \n",
              "14                 (s largest , 94)          (not always , 63)   \n",
              "15               (very strong , 89)        (very closely , 62)   \n",
              "16               (more severe , 89)      (extremely well , 61)   \n",
              "17             (so far killed , 88)           (very hard , 59)   \n",
              "18                  (very few , 86)             (as much , 52)   \n",
              "19             (still unknown , 78)            (not back , 50)   \n",
              "20            (still possible , 78)     (first federally , 50)   \n",
              "21             (back negative , 76)            (not very , 48)   \n",
              "22             (seriously ill , 76)           (even just , 46)   \n",
              "23                 (very much , 74)            (thus far , 45)   \n",
              "24                 (only able , 71)           (no longer , 44)   \n",
              "25                (very scary , 70)           (Right now , 43)   \n",
              "26               (much higher , 68)        (mostly still , 42)   \n",
              "27            (very important , 68)              (not as , 41)   \n",
              "28                (much lower , 68)     (definitely very , 39)   \n",
              "29                 (much more , 67)            (well bid , 39)   \n",
              "30              (most popular , 63)  (probably initially , 39)   \n",
              "31               (really good , 63)            (far away , 39)   \n",
              "32               (most likely , 62)          (then again , 38)   \n",
              "33          (relatively small , 62)    (still relatively , 37)   \n",
              "34              (very intense , 61)            (too soon , 35)   \n",
              "35               (more likely , 60)           (yet again , 35)   \n",
              "36                  (s likely , 59)        (together now , 32)   \n",
              "37            (most effective , 59)             (not too , 32)   \n",
              "38                (now higher , 57)          (very close , 31)   \n",
              "39                  (now more , 55)             (too far , 30)   \n",
              "40                 (far lower , 55)            (too much , 29)   \n",
              "41   (extremely well prepared , 54)     (not necessarily , 29)   \n",
              "42               (not present , 54)      (very seriously , 28)   \n",
              "43            (very difficult , 54)     (not immediately , 28)   \n",
              "44               (much larger , 54)             (Even so , 28)   \n",
              "45                (very close , 53)           (also well , 28)   \n",
              "46            (more confident , 53)            (not long , 28)   \n",
              "47               (most common , 53)      (shortly before , 28)   \n",
              "48               (very little , 52)          (n't always , 27)   \n",
              "49        (most comprehensive , 52)              (as far , 26)   \n",
              "\n",
              "                    verb phrase  \n",
              "0            (have been , 1824)  \n",
              "1           (has killed , 1778)  \n",
              "2             (has been , 1093)  \n",
              "3           (have tested , 806)  \n",
              "4          (has infected , 768)  \n",
              "5             (have died , 764)  \n",
              "6            (has spread , 745)  \n",
              "7    (have been reported , 591)  \n",
              "8   (have been confirmed , 587)  \n",
              "9              (had been , 573)  \n",
              "10          (is believed , 556)  \n",
              "11       (be quarantined , 493)  \n",
              "12                  (’ s , 483)  \n",
              "13   (have been infected , 448)  \n",
              "14            (has taken , 383)  \n",
              "15         (has reported , 379)  \n",
              "16        (was diagnosed , 346)  \n",
              "17             (be taken , 344)  \n",
              "18         (has declared , 343)  \n",
              "19        (were reported , 339)  \n",
              "20           (are taking , 334)  \n",
              "21        (was confirmed , 325)  \n",
              "22           (were taken , 322)  \n",
              "23        (were sickened , 321)  \n",
              "24         (has sickened , 315)  \n",
              "25              (has had , 308)  \n",
              "26          (are working , 281)  \n",
              "27           (were found , 268)  \n",
              "28       (were diagnosed , 260)  \n",
              "29          (be infected , 252)  \n",
              "30  (have been diagnosed , 251)  \n",
              "31           (had tested , 246)  \n",
              "32      (have originated , 244)  \n",
              "33            (was taken , 243)  \n",
              "34      (have contracted , 238)  \n",
              "35             (has said , 225)  \n",
              "36        (being treated , 225)  \n",
              "37            (are going , 215)  \n",
              "38                 (’ re , 212)  \n",
              "39            (are doing , 212)  \n",
              "40             (has made , 211)  \n",
              "41       (being infected , 210)  \n",
              "42             (has seen , 205)  \n",
              "43            (has risen , 203)  \n",
              "44           (has become , 203)  \n",
              "45         (have emerged , 200)  \n",
              "46       (have confirmed , 190)  \n",
              "47              (was put , 186)  \n",
              "48        (has confirmed , 185)  \n",
              "49         (had traveled , 177)  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 243
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9UU0bMewhGAh"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LUYVmVi3hGAi"
      },
      "source": [
        "#loop through sentence list and append to a new list \n",
        "neg_sents = []\n",
        "\n",
        "for i in indices_neg:\n",
        "    neg_sents.append(sentences[i])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OUsY4nhPhGAi"
      },
      "source": [
        "tokentext_neg = [nltk.word_tokenize(sent) for sent in neg_sents] #tokenize words within sentences\n",
        "taggedtext_neg = [nltk.pos_tag(tokens) for tokens in tokentext_neg] #apply POS tags to each token generated in the previous line"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s2pVM9vrhGAi"
      },
      "source": [
        "adjPhrases_neg = findPhrases(taggedtext_neg,\"test: {<RB.?>+<JJ.?>}\") #\"adjective phrases\" "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DC-hX3XrhGAi"
      },
      "source": [
        "#call functions on RB + RB\n",
        "adverbPhrases_neg = findPhrases(taggedtext_neg,\"test: {<RB>+<RB>}\") #\"adverb phrases\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oj1XfoKdhGAi"
      },
      "source": [
        "verbverb_neg = findPhrases(taggedtext_neg,\"test: {<VB.?>+<VB.?>}\") #verbs+verbs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dHpFgpexhGAi",
        "outputId": "9741a220-a995-4948-ac0d-bf65c725ff39"
      },
      "source": [
        "#combine all into dataframe of tuples with phrase and count\n",
        "df_neg = pd.DataFrame(list(zip(adjPhrases_neg[0], adverbPhrases_neg[0], verbverb_neg[0])), columns =['adjective phrase', 'adverb phrase','verb phrase']) \n",
        "df_neg"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>adjective phrase</th>\n",
              "      <th>adverb phrase</th>\n",
              "      <th>verb phrase</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>(so many , 125)</td>\n",
              "      <td>(right now , 323)</td>\n",
              "      <td>(has been , 1455)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>(not available , 86)</td>\n",
              "      <td>(so far , 274)</td>\n",
              "      <td>(have been , 1290)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>(Most Popular , 82)</td>\n",
              "      <td>(as well , 263)</td>\n",
              "      <td>(had been , 581)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>(safer self-quarantining , 72)</td>\n",
              "      <td>(not yet , 235)</td>\n",
              "      <td>(is happening , 489)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>(not clear , 69)</td>\n",
              "      <td>(not only , 229)</td>\n",
              "      <td>(’ s , 463)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>(very close , 65)</td>\n",
              "      <td>(not just , 187)</td>\n",
              "      <td>(is going , 344)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>(not sure , 65)</td>\n",
              "      <td>(else right , 154)</td>\n",
              "      <td>(are going , 254)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>(already fully disinfect , 63)</td>\n",
              "      <td>(not necessarily , 143)</td>\n",
              "      <td>(’ re , 228)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>(too late , 61)</td>\n",
              "      <td>(not immediately , 116)</td>\n",
              "      <td>(are getting , 206)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>(most important , 60)</td>\n",
              "      <td>(no longer , 90)</td>\n",
              "      <td>(have seen , 186)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>(too much , 54)</td>\n",
              "      <td>(as long , 89)</td>\n",
              "      <td>('re going , 184)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>(very important , 53)</td>\n",
              "      <td>(not even , 88)</td>\n",
              "      <td>(was told , 177)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>(very different , 52)</td>\n",
              "      <td>(So far , 82)</td>\n",
              "      <td>('s going , 159)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>(so much , 52)</td>\n",
              "      <td>(n't even , 80)</td>\n",
              "      <td>('ve got , 155)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>(really good , 51)</td>\n",
              "      <td>(not currently , 76)</td>\n",
              "      <td>(was going , 153)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>(much more , 51)</td>\n",
              "      <td>(still not , 69)</td>\n",
              "      <td>(have had , 134)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>(very difficult , 50)</td>\n",
              "      <td>(Right now , 66)</td>\n",
              "      <td>(be used , 132)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>(very intense , 49)</td>\n",
              "      <td>(once again , 66)</td>\n",
              "      <td>(are working , 125)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>(too close , 48)</td>\n",
              "      <td>(not really , 65)</td>\n",
              "      <td>(were told , 115)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>(too many , 46)</td>\n",
              "      <td>(already fully , 64)</td>\n",
              "      <td>(being treated , 111)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>(more likely , 44)</td>\n",
              "      <td>(m not , 62)</td>\n",
              "      <td>(has said , 107)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>(very serious , 44)</td>\n",
              "      <td>(up here , 60)</td>\n",
              "      <td>(be held , 101)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>(hugely important , 44)</td>\n",
              "      <td>(definitely not , 50)</td>\n",
              "      <td>(be done , 99)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>(very good , 43)</td>\n",
              "      <td>(certainly not , 49)</td>\n",
              "      <td>(have taken , 98)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>(then mixed , 43)</td>\n",
              "      <td>(n't really , 48)</td>\n",
              "      <td>(be given , 96)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>(more important , 41)</td>\n",
              "      <td>(somewhere else , 46)</td>\n",
              "      <td>(be allowed , 96)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>(widely available , 41)</td>\n",
              "      <td>(as soon , 45)</td>\n",
              "      <td>('ve been , 95)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>(more difficult , 41)</td>\n",
              "      <td>(very well , 44)</td>\n",
              "      <td>(have said , 91)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>(more serious , 40)</td>\n",
              "      <td>(not so , 42)</td>\n",
              "      <td>(have died , 90)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>(now open , 40)</td>\n",
              "      <td>(now only , 42)</td>\n",
              "      <td>(are trying , 88)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>(personally identifiable , 38)</td>\n",
              "      <td>(Not long , 40)</td>\n",
              "      <td>(is working , 85)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>(more familiar , 38)</td>\n",
              "      <td>(not always , 39)</td>\n",
              "      <td>('ve had , 84)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>(n't own , 37)</td>\n",
              "      <td>(still there , 39)</td>\n",
              "      <td>(be treated , 81)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>(more diagnostic , 37)</td>\n",
              "      <td>(now enough , 37)</td>\n",
              "      <td>('ve seen , 81)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>(there more , 36)</td>\n",
              "      <td>(not enough , 36)</td>\n",
              "      <td>(have been infected , 80)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>(not necessarily real-time , 36)</td>\n",
              "      <td>(not too , 36)</td>\n",
              "      <td>(have been asked , 80)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>(most effective , 36)</td>\n",
              "      <td>(as far , 35)</td>\n",
              "      <td>(been changed , 79)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>(not able , 34)</td>\n",
              "      <td>(Not only , 34)</td>\n",
              "      <td>(is getting , 79)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>(completely unacceptable , 34)</td>\n",
              "      <td>(together now , 33)</td>\n",
              "      <td>(been edited , 79)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>(fairly good , 33)</td>\n",
              "      <td>(just yet , 33)</td>\n",
              "      <td>(have made , 79)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40</th>\n",
              "      <td>(not sick , 32)</td>\n",
              "      <td>(not as , 32)</td>\n",
              "      <td>(be made , 78)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41</th>\n",
              "      <td>(too early , 32)</td>\n",
              "      <td>(not alone , 31)</td>\n",
              "      <td>('ve sent , 77)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42</th>\n",
              "      <td>(definitely not prepared , 31)</td>\n",
              "      <td>(As far , 29)</td>\n",
              "      <td>(has become , 77)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>43</th>\n",
              "      <td>(less panicked , 30)</td>\n",
              "      <td>(so long , 28)</td>\n",
              "      <td>(have been told , 77)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44</th>\n",
              "      <td>(very hands-on , 29)</td>\n",
              "      <td>(just not , 28)</td>\n",
              "      <td>(be considered , 75)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45</th>\n",
              "      <td>(publicly available , 29)</td>\n",
              "      <td>(over again , 26)</td>\n",
              "      <td>(has made , 73)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>46</th>\n",
              "      <td>(not absolute , 29)</td>\n",
              "      <td>(probably not , 25)</td>\n",
              "      <td>(have been sent , 72)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47</th>\n",
              "      <td>(very relaxed , 29)</td>\n",
              "      <td>(n't recently , 25)</td>\n",
              "      <td>(have come , 72)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>48</th>\n",
              "      <td>(not undergone , 28)</td>\n",
              "      <td>(So clearly , 25)</td>\n",
              "      <td>(do risking , 71)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49</th>\n",
              "      <td>(very prolonged , 28)</td>\n",
              "      <td>(“ Even , 25)</td>\n",
              "      <td>(had said , 70)</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                    adjective phrase            adverb phrase  \\\n",
              "0                    (so many , 125)        (right now , 323)   \n",
              "1               (not available , 86)           (so far , 274)   \n",
              "2                (Most Popular , 82)          (as well , 263)   \n",
              "3     (safer self-quarantining , 72)          (not yet , 235)   \n",
              "4                   (not clear , 69)         (not only , 229)   \n",
              "5                  (very close , 65)         (not just , 187)   \n",
              "6                    (not sure , 65)       (else right , 154)   \n",
              "7     (already fully disinfect , 63)  (not necessarily , 143)   \n",
              "8                    (too late , 61)  (not immediately , 116)   \n",
              "9              (most important , 60)         (no longer , 90)   \n",
              "10                   (too much , 54)           (as long , 89)   \n",
              "11             (very important , 53)          (not even , 88)   \n",
              "12             (very different , 52)            (So far , 82)   \n",
              "13                    (so much , 52)          (n't even , 80)   \n",
              "14                (really good , 51)     (not currently , 76)   \n",
              "15                  (much more , 51)         (still not , 69)   \n",
              "16             (very difficult , 50)         (Right now , 66)   \n",
              "17               (very intense , 49)        (once again , 66)   \n",
              "18                  (too close , 48)        (not really , 65)   \n",
              "19                   (too many , 46)     (already fully , 64)   \n",
              "20                (more likely , 44)             (m not , 62)   \n",
              "21               (very serious , 44)           (up here , 60)   \n",
              "22           (hugely important , 44)    (definitely not , 50)   \n",
              "23                  (very good , 43)     (certainly not , 49)   \n",
              "24                 (then mixed , 43)        (n't really , 48)   \n",
              "25             (more important , 41)    (somewhere else , 46)   \n",
              "26           (widely available , 41)           (as soon , 45)   \n",
              "27             (more difficult , 41)         (very well , 44)   \n",
              "28               (more serious , 40)            (not so , 42)   \n",
              "29                   (now open , 40)          (now only , 42)   \n",
              "30    (personally identifiable , 38)          (Not long , 40)   \n",
              "31              (more familiar , 38)        (not always , 39)   \n",
              "32                    (n't own , 37)       (still there , 39)   \n",
              "33            (more diagnostic , 37)        (now enough , 37)   \n",
              "34                 (there more , 36)        (not enough , 36)   \n",
              "35  (not necessarily real-time , 36)           (not too , 36)   \n",
              "36             (most effective , 36)            (as far , 35)   \n",
              "37                   (not able , 34)          (Not only , 34)   \n",
              "38    (completely unacceptable , 34)      (together now , 33)   \n",
              "39                (fairly good , 33)          (just yet , 33)   \n",
              "40                   (not sick , 32)            (not as , 32)   \n",
              "41                  (too early , 32)         (not alone , 31)   \n",
              "42    (definitely not prepared , 31)            (As far , 29)   \n",
              "43              (less panicked , 30)           (so long , 28)   \n",
              "44              (very hands-on , 29)          (just not , 28)   \n",
              "45         (publicly available , 29)        (over again , 26)   \n",
              "46               (not absolute , 29)      (probably not , 25)   \n",
              "47               (very relaxed , 29)      (n't recently , 25)   \n",
              "48              (not undergone , 28)        (So clearly , 25)   \n",
              "49             (very prolonged , 28)            (“ Even , 25)   \n",
              "\n",
              "                  verb phrase  \n",
              "0           (has been , 1455)  \n",
              "1          (have been , 1290)  \n",
              "2            (had been , 581)  \n",
              "3        (is happening , 489)  \n",
              "4                 (’ s , 463)  \n",
              "5            (is going , 344)  \n",
              "6           (are going , 254)  \n",
              "7                (’ re , 228)  \n",
              "8         (are getting , 206)  \n",
              "9           (have seen , 186)  \n",
              "10          ('re going , 184)  \n",
              "11           (was told , 177)  \n",
              "12           ('s going , 159)  \n",
              "13            ('ve got , 155)  \n",
              "14          (was going , 153)  \n",
              "15           (have had , 134)  \n",
              "16            (be used , 132)  \n",
              "17        (are working , 125)  \n",
              "18          (were told , 115)  \n",
              "19      (being treated , 111)  \n",
              "20           (has said , 107)  \n",
              "21            (be held , 101)  \n",
              "22             (be done , 99)  \n",
              "23          (have taken , 98)  \n",
              "24            (be given , 96)  \n",
              "25          (be allowed , 96)  \n",
              "26            ('ve been , 95)  \n",
              "27           (have said , 91)  \n",
              "28           (have died , 90)  \n",
              "29          (are trying , 88)  \n",
              "30          (is working , 85)  \n",
              "31             ('ve had , 84)  \n",
              "32          (be treated , 81)  \n",
              "33            ('ve seen , 81)  \n",
              "34  (have been infected , 80)  \n",
              "35     (have been asked , 80)  \n",
              "36        (been changed , 79)  \n",
              "37          (is getting , 79)  \n",
              "38         (been edited , 79)  \n",
              "39           (have made , 79)  \n",
              "40             (be made , 78)  \n",
              "41            ('ve sent , 77)  \n",
              "42          (has become , 77)  \n",
              "43      (have been told , 77)  \n",
              "44       (be considered , 75)  \n",
              "45            (has made , 73)  \n",
              "46      (have been sent , 72)  \n",
              "47           (have come , 72)  \n",
              "48          (do risking , 71)  \n",
              "49            (had said , 70)  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 249
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "87MFJ0rBhGAj"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lGLE0uqwhGAj"
      },
      "source": [
        "## LSTM "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xDGUypNahGAj"
      },
      "source": [
        "#give credit to source of tutorial (although changes were implemented)\n",
        "#https://medium.datadriveninvestor.com/deep-learning-lstm-for-sentiment-analysis-in-tensorflow-with-keras-api-92e62cde7626"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_i5sb01LhGAj"
      },
      "source": [
        "tweet_df = train_dataset[['text','airline_sentiment']] #new df with relevant info \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hwkVOZByhGAj"
      },
      "source": [
        "sentiment_label = tweet_df.airline_sentiment.factorize() #turn named sentiment into factors(0,1,2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r8leSSKkhGAk"
      },
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer #import modules \n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences #import modules \n",
        "\n",
        "#get values \n",
        "tweet = tweet_df.text.values\n",
        "tokenizer = Tokenizer(num_words=7500) #tokenizer with 7,500 size vocab\n",
        "tokenizer.fit_on_texts(tweet) #apply tokenizer\n",
        "vocab_size = len(tokenizer.word_index) + 1 #get size \n",
        "encoded_docs = tokenizer.texts_to_sequences(tweet) #encode to numbered sequence\n",
        "padded_sequence = pad_sequences(encoded_docs, maxlen=200) #filter sequence to maxlen=200"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bZld9UAphGAk",
        "outputId": "e25d2407-b350-4d25-e562-b6f6feb0c7ee"
      },
      "source": [
        "# Build the model\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM,Dense, Dropout\n",
        "from tensorflow.keras.layers import SpatialDropout1D\n",
        "from tensorflow.keras.layers import Embedding\n",
        "\n",
        "#set vector length\n",
        "embedding_vector_length = 32\n",
        "model = Sequential() #instantiate sequential model\n",
        "\n",
        "#add all layers \n",
        "\n",
        "#dropout turns off neurons to prevent overfitting \n",
        "model.add(Embedding(vocab_size, embedding_vector_length,input_length=200) )\n",
        "model.add(SpatialDropout1D(0.25))\n",
        "model.add(LSTM(50, dropout=0.5, recurrent_dropout=0.5))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "#dense layer has 3 outputs for the 3 classes \n",
        "model.add(Dense(3, activation='sigmoid'))\n",
        "model.compile(loss='SparseCategoricalCrossentropy',optimizer='adam', metrics=['accuracy'])\n",
        "print(model.summary())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_6\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_6 (Embedding)      (None, 200, 32)           504608    \n",
            "_________________________________________________________________\n",
            "spatial_dropout1d_6 (Spatial (None, 200, 32)           0         \n",
            "_________________________________________________________________\n",
            "lstm_6 (LSTM)                (None, 50)                16600     \n",
            "_________________________________________________________________\n",
            "dropout_6 (Dropout)          (None, 50)                0         \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 3)                 153       \n",
            "=================================================================\n",
            "Total params: 521,361\n",
            "Trainable params: 521,361\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8H3AVG6_hGAk",
        "outputId": "10c1f0eb-a78d-40e6-cfbd-8162b78a1356"
      },
      "source": [
        "#fit everything with validation split at 10% and run for 5 epochs\n",
        "history = model.fit(padded_sequence,sentiment_label[0],validation_split=0.1, epochs=5, batch_size=32)\n",
        "\n",
        "#validation accuracy of .8436 a massive improvment on NB"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "412/412 [==============================] - 52s 122ms/step - loss: 0.8755 - accuracy: 0.6209 - val_loss: 0.4692 - val_accuracy: 0.8374\n",
            "Epoch 2/5\n",
            "412/412 [==============================] - 53s 129ms/step - loss: 0.5721 - accuracy: 0.7709 - val_loss: 0.3805 - val_accuracy: 0.8415\n",
            "Epoch 3/5\n",
            "412/412 [==============================] - 53s 128ms/step - loss: 0.4546 - accuracy: 0.8251 - val_loss: 0.3684 - val_accuracy: 0.8456\n",
            "Epoch 4/5\n",
            "412/412 [==============================] - 52s 127ms/step - loss: 0.3945 - accuracy: 0.8503 - val_loss: 0.4093 - val_accuracy: 0.8292\n",
            "Epoch 5/5\n",
            "412/412 [==============================] - 52s 126ms/step - loss: 0.3496 - accuracy: 0.8675 - val_loss: 0.3970 - val_accuracy: 0.8436\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2YcXvO4AhGAl"
      },
      "source": [
        "#create ordered list of sentiment for getting back label from an index \n",
        "sentiment = ['neutral', 'positive', 'negative']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Otv6MjychGAl"
      },
      "source": [
        "#create function to test string\n",
        "def testString(test_word):\n",
        "    tw = tokenizer.texts_to_sequences([test_word])\n",
        "    tw = pad_sequences(tw,maxlen=200)\n",
        "    prediction = model.predict(tw)\n",
        "    print(prediction)\n",
        "    print(sentiment[np.argmax(prediction)])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A8Et0b2NhGAl",
        "outputId": "45081aad-5c2b-49c7-e534-8ee5eec5714e"
      },
      "source": [
        "testString(\"they were amazingly bad\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.44376218 0.46981794 0.5761415 ]]\n",
            "negative\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HTbROP1AhGAl",
        "outputId": "89dee76f-8e20-4302-ce2c-8dde454bac9a"
      },
      "source": [
        "testString(\"this is the best thing I have done\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.38965803 0.9004164  0.11448115]]\n",
            "positive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xnb8Qy4bhGAm",
        "outputId": "20b8d3ad-4419-4f0a-818e-da60b62461ee"
      },
      "source": [
        "testString(\"people said they were amazingly bad, but I actually really like them\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.27355775 0.3706975  0.8630587 ]]\n",
            "negative\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EaWoWrfohGAm"
      },
      "source": [
        "#import module\n",
        "from random import sample "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6BRuyjAahGAm"
      },
      "source": [
        "#randomely sample positive and negative sentences by Naive Bayes \n",
        "testNegs = sample(indices_neg,300)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4XYRPTbShGAm"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KY6_38X5hGAm"
      },
      "source": [
        "#hold LSTM predictions\n",
        "deepSents = []\n",
        "\n",
        "#loop through the negative tweets and classify them using the LSTM\n",
        "for i in testNegs:\n",
        "    tw = tokenizer.texts_to_sequences([sentences[i]])\n",
        "    tw = pad_sequences(tw,maxlen=200)\n",
        "    prediction = model.predict(tw)\n",
        "    deepSents.append(sentiment[np.argmax(prediction)])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dfgTahM9hGAn"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bQi0m86rhGAn"
      },
      "source": [
        "#find the tweets that have opposite classification as Naive Bayes \n",
        "indTest = [i for i, x in enumerate(deepSents) if x == \"positive\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wJex4r0ohGAn",
        "outputId": "bcd8bb78-6e02-459e-abc6-eee42e5d2870"
      },
      "source": [
        "#tagged as positive by LSTM and negative by NB\n",
        "for i in indTest:\n",
        "    print()\n",
        "    print(sentences[i])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Toronto Public Health will monitor the patient while he continues to recover at home, where his wife is also in self-isolation.\n",
            "\n",
            "Dr Tedros, speaking at the press conference in Geneva, described the virus as an \"unprecedented outbreak\" that has been met with an \"unprecedented response\".\n",
            "\n",
            "The WHO declares a Public Health Emergency of International Concern when there is \"an extraordinary event which is determined … to constitute a public health risk to other states through the international spread of disease\".\n",
            "\n",
            "Although questions have been raised about transparency, the WHO has praised China's handling of the outbreak.\n",
            "\n",
            "The province of 60 million people is home to Wuhan, the heart of the outbreak.\n",
            "\n",
            "Peter Morris, chief economist at Ascend by Cirium, said: “Cirium data clearly shows the dramatic impact that coronavirus is having, with nearly 10,000 scheduled flights to, from and within China being suspended between January 23 and 28.\n",
            "\n",
            "Beijing, which has only just started to mend tattered trade ties with the US, called that move “truly mean” given the World Health Organization had commended its containment efforts and not recommended travel or trade curbs.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5YxXCI8qhGAn"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QcKS0qR-hGAn"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3HSvTwFdhGAn"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z39l1U4vhGAn"
      },
      "source": [
        "#same as above but for positives \n",
        "testPos = sample(indices_pos,30)\n",
        "\n",
        "deepSents = []\n",
        "\n",
        "for i in testPos:\n",
        "    tw = tokenizer.texts_to_sequences([sentences[i]])\n",
        "    tw = pad_sequences(tw,maxlen=200)\n",
        "    prediction = model.predict(tw)\n",
        "    deepSents.append(sentiment[np.argmax(prediction)])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wOXaqaA0hGAo",
        "outputId": "bdd0be1b-3b76-4dae-eb4d-48eac1137b22"
      },
      "source": [
        "#tagged as negative by LSTM and positive by NB\n",
        "\n",
        "indTest = [i for i, x in enumerate(deepSents) if x == \"negative\"]\n",
        "for i in indTest:\n",
        "    print()\n",
        "    print(sentences[i])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "All district hospitals will have five beds isolated for patients carrying the virus.\n",
            "\n",
            "104 Arogya Sahayavani helpline run by the Health Department will take all calls related to the virus.\n",
            "\n",
            "The government making sure that the new coronavirus does not make its way to the country.\n",
            "\n",
            "Apart from more people falling sick (as bad as that is), is there a more fundamental concern that if it runs wild in a less developed country, it'll mutate into something more dangerous?\n",
            "\n",
            "Among the major miners, Fortescue Metals is advancing more than 1 percent, BHP is adding 1 percent and Rio Tinto is up 0.2 percent.\n",
            "\n",
            "Among the big four banks, ANZ Banking, Commonwealth Bank and National Australia Bank are rising in a range of 0.4 percent to 0.5 percent, while Westpac is edging up 0.1 percent.\n",
            "\n",
            "Commonwealth Bank, which reports half-year results on February 12, said it will make a provision of A$83 million for insurance claims related to the recent bushfires in Australia.\n",
            "\n",
            "Gold miners are weak despite gold prices rising overnight.\n",
            "\n",
            "ResMed reported a 13 percent increase in revenue for the second quarter from last year, reflecting growing demand for masks and other medical accessories.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WySqYpayhGAo"
      },
      "source": [
        "## Interpretation of the results\n",
        "\n",
        "### NB 1 \n",
        "Both Naive Bayes model are trained on only the airline data. The first Naive Bayes model used is a Complement Naive Bayes model.Documentation states that the Complement Naive Bayes model is built to handle unbalanced datasets. It seemed like the correct choice for this problem. The initial model is only unigrams. For a unigram to be admitted to be used as a predictor it must show up in at least 2,000 documents. This helps the model not overfit rare words. The hope in this analysis is to not overfit sentiment based on the name of the airline. Based on this limit, there are 891 unigrams chosen as predictors. \n",
        "\n",
        "Cross validation is essential for truly measuring the validity of the model. Five fold cross validation is used and the rates are as follows. \n",
        "\n",
        "[0.67588798, 0.64378415, 0.60382514, 0.72096995, 0.70696721]\n",
        "\n",
        "As we can see the best fold is .72 or 72% correct, but the worst is .60 or 60%. This is very intriguing, however, not for good reason. It shows that there is a large variability based on which fold is used. The model is potentially somewhat unstable. \n",
        "\n",
        "The average of these is .67. Although this seems bad, it is interesting that a simple Naive Bayes classifier with only 891 unigrams can accurately classify a 3 class problem 67% of the time. It does need improvement. \n",
        "\n",
        "\n",
        "### NB 2 \n",
        "\n",
        "The second Naive Bayes model is also a Complement Naive Bayes model. However, this time we use unigrams, bigrams, and trigrams. For any of these to be admitted as a predictor they must be found in at least 1000 of the documents. This also should help preventing overfitting on something like airline names. The model results in \n",
        "\n",
        "The cross validation scores for this are as followed. \n",
        "\n",
        "[0.69740437, 0.6875 , 0.67657104, 0.76058743, 0.73155738]\n",
        "\n",
        "The highest fold is .76 or 76%. The lowest fold is .67 or 67%. As compared to the previous model, the folds as a whole have a lower standard deviation. This is better. The reason is that there is less variability meaning that our model is more stable and consistent across each fold. The average rate is .71 or 71%. Still much room for improvement but this is an interesting baseline to compare against the LSTM which is the main model of this report. \n",
        "\n",
        "It is important to reiterate that both Naive Bayes model are trained on only the airline data. \n",
        "\n",
        "### Phrases: Sentiment Predictions (Naive Bayes)\n",
        "\n",
        "We will use the better of the two Naive Bayes models to make predictions on every single sentence in the covid set (1/4 of actual data). This allows us to view the counts of positive, negative, and neutral sentiment for each sentence in the covid tweet data. The result is 171,743 positive sentences, 107,066 negative sentences, and 304,641 neutral sentences. This is clearly imbalanced. Furthermore, summing all of these results in 583,450 sentences, showing that we have made the correct number of predictions. \n",
        "\n",
        "From here, we build a table that has title, author, country, negative, neutral, and positive as the columns. This allows us to calculate the conditional mean number of positive and negative sentences given country. This however, is more involved than it may seem. First, we must get the conditional averages for positive and negative sentiment. However, we cannot simply order these and analyze that result. The reason is that these means only regard the single sentiment type. For example, a country may have the largest average amount of positive sentiment sentences, but it also may have many negative. Therefore, I take the difference between the average positive and negative sentences given the country. The results are very interesting. BR or Brazil has by far on average the most positive sentiment. This is if we make the assumption that our model is correctly classifying the sentiment. The most negative country on average is BA or Bosnia and Herzegovina. Brazils difference score is 7.3 and Bosnia’s is -17. For reference, the global average sentiment difference is -2.2. The United States scores just under the average with a score of -3. \n",
        "\n",
        "---\n",
        "\n",
        "##### Phrase extraction and issues \n",
        "\n",
        "\n",
        "Next, we extract Adverb, Verb, and Adjective phrases for the all the positive and negative sentences in the covid tweet data set. It is important to note here some issues with the results. There were clearly tokenization issues that need deeper investigation in order to fix. These are things like 's being counted as a word. Investigation into the tokenizer is needed to fix and investigate this issue and come to a resolution. Furthermore, this happens in all three phrase groups. Furthermore, the data is never lowercased. This means we have repeated phrases where its counting lower case version and upper case version as different. This also needs to be fixed moving forward. For the purpose of this report we still were able to obtain very interesting results that are worth exploring. \n",
        "\n",
        "#### Adjective Phrases (Positive and Negative) \n",
        "\n",
        "Through living with covid and being a part of this history, we understand that positive covid results often come from early action. Negative results also often come from late action or inaction all together. Amazingly we can see this in the adjective phrases. Many of the top ten adjective phrases for the positive sentences have to do with early action or preparedness. The 8th phrase is, (extremely well-prepared , 129) and the 7th is (very early , 143). This is a stark contrast to the negative adjective phrases, one of the top ten are (too late , 61). Another interesting example of this is that the positive list has the phrase (too early , 143) while the negative has the phrase (too late , 61). It is important to note that there are many similarities between both lists. For example, the positive list of adjective phrases has phrases like (too much , 147), (so far killed , 88), (more serious , 117), and (too much , 147). These could all be interpreted as negative. This is to be expected though as the subject of covid itself is very negative. A distinct theme found in the negative list of adjective phrases is that of being unknown. For example, some of the top adjective phrases in the negative list are (not clear , 69), (not available , 86), and (not sure , 65). Although both lists are negative, we can draw some interesting difference as described above. \n",
        "\n",
        "\n",
        "#### Adverb Phrases (Positive and Negative) \n",
        "\n",
        "The adverb phrases as a whole are less interesting than the adjective phrases. This is because the lists are extremely similar. More preprocessing may help this but it just may be that the similarities in speech between positive and negative in this specific case is small. For example, the first 5 of 6 phrases in both the negative and positive adverb lists are the same, although they are in different orders. The only difference is the negative list has (not yet , 235) while the positive does not. Other than that, 5 of the first 6 are the same. These are, (so far , 2111), (as well , 2010), (not just , 217), (not only , 201), and (right now , 190). The fact that the lists are similar imply that these adverbs are simply a part of the speech used that are uncorrelated to sentiment. This is an opportunity to also point out an issue discussed above. The positive list has (so far , 2111) and (So far , 586). This should be fixed by lowercasing the text and adding that to the data pipeline in the preprocessing stage. \n",
        "\n",
        "\n",
        "\n",
        "#### Verb Phrases (Positive and Negative) \n",
        "\n",
        "The verb phrases, especially in the negative list, have many issues. There are many examples like, ('s going , 159) that are not correct. This is another example of something that should be fixed in the tokenization process. Although there are difference between the lists, the positive list is very negative. This makes it difficult to asses how they were tagged as positive while focusing on the verb phrases. Some of these include, (has killed , 1778), (has infected , 768), (have died , 764), and (have been reported , 591). Needles to say these do not seem positive in nature. The negative list includes things like (has been , 1455), (have been , 1290), and (is happening , 489). These results imply that some preprocessing should have been done to remove some of these more useless words. \n",
        "\n",
        "An important insight here is that we can often learn from what we do not find. The fact that those verbs mentioned above, the ones in the positive list but appeared to be very negative, are so prevalent in the positive list actually tells us something interesting. The Naive Bayes model is trained on the airline data. The airline data most likely does not include words like killed and infected. Therefore, when the algorithm comes across them in the testing set it’s not entirely sure what to do with them. This fact calls into question the ability of being able to generalize a model trained on one dataset to another that is very different. \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "### Deep RNN: LSTM \n",
        "\n",
        "This report really shows how much better and more advanced an LSTM is at this task than the above models. The best cross validation Naive Bayes model was about .72 or 72%. On the validation set the LSTM performs at .8436 or 84.36%. This is a massive improvement on what was previously achieved. After seeing this result on the validation set, I started testing random strings to see the label. I was initially impressed, it was able to classify 'they were amazingly bad' as 'negative'. This is impressive because of the amazing and the bad. With very high confidence it classified 'this is the best thing I have done' as 'positive'. However, I was able to confuse the model. I created the string, \"people said they were amazingly bad, but I actually really like them\". This was classified as 'negative' with 86% certainty. \n",
        "\n",
        "From here I tried predicting every single sentence with the LSTM that was predicted by the Naive Bayes models, however, it was computationally too much. Therefore, I simply sampled some positive and negative sentences to see if the LSTM would classify them with the opposite sentiment. This was actually often the case. For example, the string 'Although questions have been raised about transparency, the WHO has praised China's handling of the outbreak.', was classified as 'positive' by the LSTM and 'negative' by the Naive Bayes model. In my opinion, the LSTM gets this right. On the other hand, the string 'ResMed reported a 13 percent increase in revenue for the second quarter from last year, reflecting growing demand for masks and other medical accessories.' is classified as 'negative', by the LSTM and 'positive' by the Naive Bayes. In this case, I actually think the Naive Bayes model is correct. \n",
        "\n",
        "In conclusion, I think that training the models on more relevant data is desirable. Furthermore, I wanted to evaluate these models without sentiment lexicons. Using them would help eliminate the vocabulary issues. There are words in one corpus that are not in the other. If we used an external lexicon this would remedy the issue. However, I wanted to see the performance with only relying on the vocabulary itself. "
      ]
    }
  ]
}